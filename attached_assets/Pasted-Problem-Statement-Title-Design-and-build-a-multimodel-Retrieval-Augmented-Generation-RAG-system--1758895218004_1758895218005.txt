Problem Statement Title:	
Design and build a multimodel Retrieval-Augmented Generation (RAG) system leveraging a Large Language Model (LLM) for OFFLINE mode that can ingest, index, and query diverse data formats such as DOC, PDF, Images and voice recordings within a unified semantic retrieval framework

Background
Centre routinely handles diverse data types: PDF, DOC, Images, screenshots, recorded calls, and free-form notes. Traditional search tools struggle with cross‑format understanding, often isolating text, image, and audio searches. Retrieval‑Augmented Generation (RAG) can address this by retrieving relevant data and grounding LLM outputs thus enhancing accuracy, reducing effort & time, and enabling content transparency. Multimodal RAG can extend this fusion to include images & audio, thus creating richer, context-aware intelligence.

Description
SIH Participants are to build a multimodal RAG-based system that:
Ingests multimodal inputs – Extract textual content from DOCX/PDF, generate embeddings for images, and perform speech-to-text conversion for audio.
Indexes all modalities in a shared vector space for seamless semantic retrieval.
Supports natural-language queries, retrieving relevant text snippets, images, and audio segments.
Generates grounded summaries or answers, integrating retrieved context via LLM.
Establishes cross-format links, e.g., connecting an audio transcript segment to a cited paragraph and screenshot.

Expected Solution
Unified Query Interface
● A simple chat or search box where users ‘type’ questions in plain language (e.g. “Show me the report that has a description about international development in 2024 OR show the report that references the screenshot taken at 14:32”).
● Optional - Support for different input modalities: other than text input - upload DOCX/PDF, drag‑and‑drop images, attach audio files, or speak their query.
Semantic & Cross‑Modal Search
● Text-to-image search: Type a query like “email screenshot” to retrieve relevant images alongside matching text passages.
● Image-to-text search: Upload or select an image (e.g. screenshot) and surface relevant documents or audio transcript snippets that semantically match it.
● Optional - Audio-to-others: play an audio clip, then retrieve related text/ documents/ images that match what was spoken.
Citation Transparency & Source Navigation
● Every answer includes numbered citations linking back to source files.
● Users can expand citations to open the original document, view full transcript segments, or inspect image metadata.